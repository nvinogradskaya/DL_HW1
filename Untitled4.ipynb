{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V28",
      "authorship_tag": "ABX9TyOqDRxg8zg2mBBJRr8AHNBy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nvinogradskaya/DL_HW1/blob/main/Untitled4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install h3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ki4vshRPYpPc",
        "outputId": "6c5d517a-3cd4-4ffb-cdbf-06db653c8223"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting h3\n",
            "  Downloading h3-4.2.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
            "Downloading h3-4.2.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: h3\n",
            "Successfully installed h3-4.2.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install tensorflow"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TKqnJS4NuiBC",
        "outputId": "1b9b158c-2b2f-4a07-cd07-dfad2812fdb0"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow\n",
            "  Downloading tensorflow-2.19.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
            "Collecting astunparse>=1.6.0 (from tensorflow)\n",
            "  Downloading astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
            "Collecting flatbuffers>=24.3.25 (from tensorflow)\n",
            "  Downloading flatbuffers-25.2.10-py2.py3-none-any.whl.metadata (875 bytes)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Collecting google-pasta>=0.1.1 (from tensorflow)\n",
            "  Downloading google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
            "Collecting libclang>=13.0.0 (from tensorflow)\n",
            "  Downloading libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl.metadata (5.2 kB)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (5.29.4)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.13.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.71.0)\n",
            "Collecting tensorboard~=2.19.0 (from tensorflow)\n",
            "  Downloading tensorboard-2.19.0-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: numpy<2.2.0,>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.0.2)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.13.0)\n",
            "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.5.1)\n",
            "Collecting tensorflow-io-gcs-filesystem>=0.23.1 (from tensorflow)\n",
            "  Downloading tensorflow_io_gcs_filesystem-0.37.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (14 kB)\n",
            "Collecting wheel<1.0,>=0.23.0 (from astunparse>=1.6.0->tensorflow)\n",
            "  Downloading wheel-0.45.1-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (14.0.0)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.0.9)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.15.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.4.26)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/lib/python3/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.3.6)\n",
            "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard~=2.19.0->tensorflow)\n",
            "  Downloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl.metadata (1.1 kB)\n",
            "Collecting werkzeug>=1.0.1 (from tensorboard~=2.19.0->tensorflow)\n",
            "  Downloading werkzeug-3.1.3-py3-none-any.whl.metadata (3.7 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
            "Downloading tensorflow-2.19.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (644.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m644.9/644.9 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
            "Downloading flatbuffers-25.2.10-py2.py3-none-any.whl (30 kB)\n",
            "Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl (24.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.5/24.5 MB\u001b[0m \u001b[31m82.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboard-2.19.0-py3-none-any.whl (5.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m114.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorflow_io_gcs_filesystem-0.37.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.1/5.1 MB\u001b[0m \u001b[31m109.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl (6.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m122.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading werkzeug-3.1.3-py3-none-any.whl (224 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading wheel-0.45.1-py3-none-any.whl (72 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: libclang, flatbuffers, wheel, werkzeug, tensorflow-io-gcs-filesystem, tensorboard-data-server, google-pasta, tensorboard, astunparse, tensorflow\n",
            "Successfully installed astunparse-1.6.3 flatbuffers-25.2.10 google-pasta-0.2.0 libclang-18.1.1 tensorboard-2.19.0 tensorboard-data-server-0.7.2 tensorflow-2.19.0 tensorflow-io-gcs-filesystem-0.37.1 werkzeug-3.1.3 wheel-0.45.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_d0AEijdSv40",
        "outputId": "d4480ce5-a9a4-43fd-eb13-39cdba90efac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import uuid\n",
        "import shutil\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle\n",
        "import h3\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import (Input, Dense, Concatenate, Dropout, LayerNormalization,\n",
        "                                     LSTM, Add, MultiHeadAttention, GlobalAveragePooling1D, RepeatVector)\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_USERS = 3\n",
        "SEQ_LENGTH = 10\n",
        "PRED_LENGTH = 10\n",
        "EMBEDDING_DIM = 16\n",
        "HIDDEN_DIM = 64\n",
        "BATCH_SIZE = 128\n",
        "EPOCHS = 10\n",
        "DATA_PATH = \"/content/drive/My Drive/Colab Notebooks/Data/\"\n",
        "SAVE_PATH = \"/content/drive/My Drive/Colab Notebooks/my-model-v3/\"\n",
        "SEQ_SAVE_PATH = os.path.join(SAVE_PATH, 'sequences/')\n",
        "os.makedirs(SAVE_PATH, exist_ok=True)\n",
        "os.makedirs(SEQ_SAVE_PATH, exist_ok=True)\n",
        "features = ['lat', 'lon', 'alt', 'hour_sin', 'hour_cos', 'day_sin', 'day_cos']"
      ],
      "metadata": {
        "id": "zUyJIoAZXJVO"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------- H3 ----------------------\n",
        "def latlon_to_h3(lat, lon, resolution):\n",
        "    return h3.latlng_to_cell(lat, lon, resolution)\n",
        "\n",
        "def add_h3_indices(df):\n",
        "    df['h3_500m'] = df.apply(lambda row: latlon_to_h3(row['lat'], row['lon'], 8), axis=1)  # Уровень 8 (~500м)\n",
        "    df['h3_5m'] = df.apply(lambda row: latlon_to_h3(row['lat'], row['lon'], 14), axis=1)   # Уровень 14 (~5м)\n",
        "    return df"
      ],
      "metadata": {
        "id": "oMXzhml4XWJ1"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_and_preprocess_data(data_path, max_users=MAX_USERS):\n",
        "    # Загрузка сырых данных\n",
        "    data = []\n",
        "    user_dirs = sorted(os.listdir(data_path))[:max_users]\n",
        "\n",
        "    for user in user_dirs:\n",
        "        traj_dir = os.path.join(data_path, user, 'Trajectory')\n",
        "        traj_files = sorted([f for f in os.listdir(traj_dir) if f.endswith('.plt')])\n",
        "\n",
        "        for traj_file in traj_files:\n",
        "            df = pd.read_csv(\n",
        "                os.path.join(traj_dir, traj_file),\n",
        "                skiprows=6, header=None, usecols=[0, 1, 3, 5, 6],\n",
        "                names=['lat', 'lon', 'alt', 'date', 'time']\n",
        "            )\n",
        "            df['user'] = user\n",
        "            data.append(df)\n",
        "\n",
        "    # Объединение и предобработка\n",
        "    df = pd.concat(data, ignore_index=True)\n",
        "\n",
        "    # Обработка времени\n",
        "    df['datetime'] = pd.to_datetime(df['date'] + ' ' + df['time'])\n",
        "    df.sort_values(by=['user', 'datetime'], inplace=True)\n",
        "\n",
        "    # Фильтрация данных\n",
        "    df = df[(df['lat'] != 0) & (df['lon'] != 0)].ffill()\n",
        "\n",
        "    # Добавление H3 индексов\n",
        "    df = add_h3_indices(df)  # Уровни 8 и 14\n",
        "\n",
        "    # Нормализация координат относительно H3 ячеек\n",
        "    def normalize_coords(row, resolution):\n",
        "    # Добавляем 'm' к resolution только в имени столбца\n",
        "        cell_center = h3.cell_to_latlng(row[f'h3_{resolution}m'])  # Теперь resolution передаётся без 'm'\n",
        "        local_lat = row['lat'] - cell_center[0]\n",
        "        local_lon = row['lon'] - cell_center[1]\n",
        "        return pd.Series([local_lat, local_lon])\n",
        "\n",
        "    # Было:\n",
        "    # Стало:\n",
        "    df[['local_lat_500m', 'local_lon_500m']] = df.apply(\n",
        "        normalize_coords, args=('500',), axis=1  # Убрали 'm' из аргумента\n",
        "    )\n",
        "    df[['local_lat_5m', 'local_lon_5m']] = df.apply(\n",
        "        normalize_coords, args=('5',), axis=1    # Убрали 'm' из аргумента\n",
        "    )\n",
        "\n",
        "    # Скалирование основных признаков\n",
        "    scaler = StandardScaler()\n",
        "    df[['lat', 'lon', 'alt']] = scaler.fit_transform(df[['lat', 'lon', 'alt']])\n",
        "\n",
        "    # Временные признаки\n",
        "    df['hour_sin'] = np.sin(2 * np.pi * df['datetime'].dt.hour / 24)\n",
        "    df['hour_cos'] = np.cos(2 * np.pi * df['datetime'].dt.hour / 24)\n",
        "    df['day_sin'] = np.sin(2 * np.pi * df['datetime'].dt.dayofweek / 7)\n",
        "    df['day_cos'] = np.cos(2 * np.pi * df['datetime'].dt.dayofweek / 7)\n",
        "\n",
        "    # Кодирование H3 индексов для макромодели\n",
        "    le_500 = LabelEncoder()\n",
        "    df['h3_500m_encoded'] = le_500.fit_transform(df['h3_500m'])\n",
        "\n",
        "    # Кодирование пользователей\n",
        "    user_ids = {user: idx for idx, user in enumerate(df['user'].unique())}\n",
        "    df['user_id'] = df['user'].map(user_ids)\n",
        "\n",
        "    return df, user_ids, scaler, le_500"
      ],
      "metadata": {
        "id": "AuP70P64XZlL"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------- Генерация последовательностей ----------------------\n",
        "def create_sequences_and_save(df, user_ids, seq_length, test_size=0.3, save_path='./seq_data'):\n",
        "    os.makedirs(save_path, exist_ok=True)\n",
        "    features = ['lat', 'lon', 'alt', 'hour_sin', 'hour_cos', 'day_sin', 'day_cos']\n",
        "    targets = ['lat', 'lon']\n",
        "\n",
        "    for user, user_df in df.groupby('user'):\n",
        "        uid = user_ids[user]\n",
        "        user_df = user_df.reset_index(drop=True)\n",
        "        split_idx = int(len(user_df) * (1 - test_size))\n",
        "        if split_idx <= seq_length:\n",
        "            continue\n",
        "\n",
        "        def save_chunk(X, y, is_train):\n",
        "            suffix = 'train' if is_train else 'test'\n",
        "            chunk_id = uuid.uuid4().hex\n",
        "            np.savez_compressed(\n",
        "                os.path.join(save_path, f'user_{uid}_{suffix}_{chunk_id}.npz'),\n",
        "                X=X, y=y, user_id=uid\n",
        "            )\n",
        "\n",
        "        def process_chunk(data, is_train=True):\n",
        "            data_values = data[features].values\n",
        "            X = np.lib.stride_tricks.sliding_window_view(data_values, (seq_length + 1, data_values.shape[1])).squeeze()\n",
        "            X = X[:, :-1]\n",
        "            y = data[targets].values[seq_length:]\n",
        "            for i in range(0, len(X), 1000):\n",
        "                save_chunk(X[i:i+1000], y[i:i+1000], is_train)\n",
        "\n",
        "        process_chunk(user_df.iloc[:split_idx], True)\n",
        "        process_chunk(user_df.iloc[split_idx-seq_length:], False)"
      ],
      "metadata": {
        "id": "MF2PagXbXct9"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------- Загрузка последовательностей ----------------------\n",
        "def load_all_sequences_from_disk(save_path):\n",
        "    X_train, X_test, y_train, y_test, users_train, users_test = [], [], [], [], [], []\n",
        "    for fname in sorted(os.listdir(save_path)):\n",
        "        if not fname.endswith('.npz'):\n",
        "            continue\n",
        "        split_type = 'train' if 'train' in fname else 'test'\n",
        "        uid = int(fname.split('_')[1])\n",
        "        data = np.load(os.path.join(save_path, fname))\n",
        "        X, y = data['X'], data['y']\n",
        "        if split_type == 'train':\n",
        "            X_train.append(X); y_train.append(y); users_train.append(np.full(len(X), uid))\n",
        "        else:\n",
        "            X_test.append(X); y_test.append(y); users_test.append(np.full(len(X), uid))\n",
        "    return (\n",
        "        np.concatenate(X_train), np.concatenate(X_test),\n",
        "        np.concatenate(y_train), np.concatenate(y_test),\n",
        "        np.concatenate(users_train), np.concatenate(users_test)\n",
        "    )"
      ],
      "metadata": {
        "id": "4TXpRTSVXgPA"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------- Контрастивная модель ----------------------\n",
        "def contrastive_model(input_shape, embedding_dim):\n",
        "    inp = Input(shape=input_shape)\n",
        "    x = LSTM(32)(inp)\n",
        "    x = Dense(embedding_dim)(x)\n",
        "    return Model(inputs=inp, outputs=x)\n",
        "\n",
        "def triplet_loss_fn(a, p, n, margin=1.0):\n",
        "    ap_dist = tf.reduce_sum(tf.square(a - p), axis=1)\n",
        "    an_dist = tf.reduce_sum(tf.square(a - n), axis=1)\n",
        "    return tf.reduce_mean(tf.maximum(ap_dist - an_dist + margin, 0.0))\n",
        "\n",
        "def create_triplets(X, user_ids):\n",
        "    anchors, positives, negatives = [], [], []\n",
        "    for uid in np.unique(user_ids):\n",
        "        same_user_idx = np.where(user_ids == uid)[0]\n",
        "        diff_user_idx = np.where(user_ids != uid)[0]\n",
        "        if len(same_user_idx) < 2:\n",
        "            continue\n",
        "        for i in range(min(len(same_user_idx) - 1, 100)):\n",
        "            a_idx, p_idx = same_user_idx[i], same_user_idx[i+1]\n",
        "            n_idx = np.random.choice(diff_user_idx)\n",
        "            anchors.append(X[a_idx])\n",
        "            positives.append(X[p_idx])\n",
        "            negatives.append(X[n_idx])\n",
        "    return np.array(anchors), np.array(positives), np.array(negatives)\n"
      ],
      "metadata": {
        "id": "HC3-SlkwXjUR"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------- Двухуровневая модель ----------------------\n",
        "def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0):\n",
        "    x = MultiHeadAttention(num_heads=num_heads, key_dim=head_size, dropout=dropout)(inputs, inputs)\n",
        "    x = Add()([x, inputs])\n",
        "    x = LayerNormalization(epsilon=1e-6)(x)\n",
        "    ff = Dense(ff_dim, activation='relu')(x)\n",
        "    ff = Dense(inputs.shape[-1])(ff)\n",
        "    x = Add()([x, ff])\n",
        "    x = LayerNormalization(epsilon=1e-6)(x)\n",
        "    return x\n",
        "\n",
        "# ---------------------- Макромодель (предсказывает следующую ячейку 500м) ----------------------\n",
        "# ---------------------- Макромодель (предсказание H3-ячейки 500м) ----------------------\n",
        "def build_macro_model(input_shape, num_classes):\n",
        "    seq_input = Input(shape=input_shape)\n",
        "    x = LSTM(64)(seq_input)\n",
        "    output = Dense(num_classes, activation='softmax', name='macro_output')(x)  # <- Явное имя\n",
        "    return Model(inputs=seq_input, outputs=output)\n",
        "\n",
        "# ---------------------- Новая микромодель (seq2seq с вниманием) ----------------------\n",
        "def build_micro_model(input_shape, num_macro_features):\n",
        "    # Энкодер\n",
        "    encoder_inputs = Input(shape=input_shape)\n",
        "    encoder_lstm = LSTM(HIDDEN_DIM, return_sequences=True, return_state=True)\n",
        "    encoder_outputs, state_h, state_c = encoder_lstm(encoder_inputs)\n",
        "\n",
        "    # Механизм внимания\n",
        "    attention = MultiHeadAttention(num_heads=2, key_dim=HIDDEN_DIM)\n",
        "    context_vector = attention(encoder_outputs, encoder_outputs)\n",
        "\n",
        "    # Декодер с объединением макропризнаков\n",
        "    macro_features = Input(shape=(num_macro_features,))\n",
        "    decoder_input = Concatenate()([context_vector[:, -1, :], macro_features])\n",
        "    decoder_input = RepeatVector(input_shape[0])(decoder_input)  # Для последовательности\n",
        "\n",
        "    decoder_lstm = LSTM(HIDDEN_DIM, return_sequences=True)\n",
        "    decoder_outputs = decoder_lstm(decoder_input, initial_state=[state_h, state_c])\n",
        "\n",
        "    # Выходной слой\n",
        "    output = Dense(2, activation='linear', name='micro_output')(decoder_outputs)  # <- Явное имя\n",
        "    return Model(inputs=[encoder_inputs, macro_features], outputs=output)\n",
        "\n",
        "\n",
        "# ---------------------- Модифицированная объединенная модель ----------------------\n",
        "# ---------------------- Кастомные слои для оберток ----------------------\n",
        "class ArgMaxLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, axis=-1, **kwargs):\n",
        "        super(ArgMaxLayer, self).__init__(**kwargs)\n",
        "        self.axis = axis\n",
        "\n",
        "    def call(self, inputs):\n",
        "        return tf.argmax(inputs, axis=self.axis)\n",
        "\n",
        "# ---------------------- Кастомный слой для преобразования H3-идентификаторов в координаты ----------------------\n",
        "class H3CellToCoordLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, le_500, **kwargs):\n",
        "        super(H3CellToCoordLayer, self).__init__(**kwargs)\n",
        "        self.le_500 = le_500\n",
        "        self.centers_tensor = self._build_cell_mapping()\n",
        "\n",
        "    def _build_cell_mapping(self):\n",
        "        cell_ids = self.le_500.classes_\n",
        "        centers = [h3.cell_to_latlng(cell) for cell in cell_ids]\n",
        "        return tf.constant(centers, dtype=tf.float32)  # Тензор [num_cells, 2]\n",
        "\n",
        "    def call(self, inputs):\n",
        "        indices = tf.cast(inputs, tf.int32)  # Преобразуем индексы в int32\n",
        "        return tf.gather(self.centers_tensor, indices)  # Векторизованный поиск\n",
        "\n",
        "# ---------------------- Модифицированная объединенная модель ----------------------\n",
        "def build_dual_scale_model(macro_input_shape, micro_input_shape, num_macro_classes, embedding_dim, le_500):\n",
        "    # Явно указываем имена входных слоев\n",
        "    macro_input = Input(shape=macro_input_shape, name='macro_input')\n",
        "    macro_output = build_macro_model(macro_input_shape, num_macro_classes)(macro_input)\n",
        "\n",
        "    # Явно указываем имена для микромодели и пользовательского ввода\n",
        "    micro_input = Input(shape=micro_input_shape, name='micro_input')\n",
        "    user_input = Input(shape=(embedding_dim,), name='user_input')\n",
        "\n",
        "    # Остальной код остается без изменений\n",
        "    cell_ids = ArgMaxLayer(axis=-1)(macro_output)\n",
        "    cell_centers = H3CellToCoordLayer(le_500)(cell_ids)\n",
        "    combined_features = Concatenate()([cell_centers, user_input])\n",
        "    micro_model = build_micro_model(micro_input_shape, num_macro_features=combined_features.shape[-1])\n",
        "    micro_output = micro_model([micro_input, combined_features])\n",
        "\n",
        "    return Model(\n",
        "        inputs=[macro_input, micro_input, user_input],\n",
        "        outputs=[macro_output, micro_output],  # Используются явные имена\n",
        "        name='DualScaleModel'\n",
        "    )"
      ],
      "metadata": {
        "id": "tcoOKwosYI_z"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CombinedDataGenerator(tf.keras.utils.Sequence):\n",
        "    def __init__(self, X_macro, X_micro, user_embeddings, y, batch_size=128):\n",
        "        self.X_macro = X_macro\n",
        "        self.X_micro = X_micro\n",
        "        self.user_embeddings = user_embeddings\n",
        "        self.y = y\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "    def __len__(self):\n",
        "        return int(np.ceil(len(self.X_macro) / self.batch_size))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        batch_X_macro = self.X_macro[index*self.batch_size : (index+1)*self.batch_size]\n",
        "        batch_X_micro = self.X_micro[index*self.batch_size : (index+1)*self.batch_size]\n",
        "        batch_user = self.user_embeddings[index*self.batch_size : (index+1)*self.batch_size]\n",
        "        batch_y = self.y[index*self.batch_size : (index+1)*self.batch_size]\n",
        "        return (\n",
        "            {\n",
        "                'macro_input': batch_X_macro,\n",
        "                'micro_input': batch_X_micro,\n",
        "                'user_input': batch_user\n",
        "            },\n",
        "            {\n",
        "                'macro_output': np.zeros(len(batch_X_macro)),\n",
        "                'micro_output': batch_y\n",
        "            }\n",
        "        )"
      ],
      "metadata": {
        "id": "L3pmi0S9YNJv"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------- Метрики ----------------------\n",
        "def evaluate_model(model, X_test, user_embeddings_test, y_test):\n",
        "    macro_pred, micro_pred = model.predict([X_test, user_embeddings_test], batch_size=128)\n",
        "\n",
        "    # ADE: средняя ошибка по всем предсказанным точкам\n",
        "    ade = np.mean(np.linalg.norm(micro_pred - y_test, axis=1))\n",
        "\n",
        "    # FDE: ошибка только для последней точки\n",
        "    fde = np.mean(np.linalg.norm(micro_pred[:, -1] - y_test[:, -1], axis=1))  # <- Исправлено\n",
        "\n",
        "    dist = np.linalg.norm(micro_pred - y_test, axis=1)\n",
        "    within_100m = np.mean(dist < 0.01) * 100\n",
        "    print(f\"ADE: {ade:.4f} | FDE: {fde:.4f} | <100m: {within_100m:.2f}%\")\n",
        "    return ade, fde, within_100m"
      ],
      "metadata": {
        "id": "dw4tegf0YP5x"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------- Запуск pipeline ----------------------\n",
        "df, user_ids, scaler, le_500 = load_and_preprocess_data(DATA_PATH)"
      ],
      "metadata": {
        "id": "2VXfBfcyYTDC"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "shutil.rmtree(SEQ_SAVE_PATH, ignore_errors=True)\n",
        "os.makedirs(SEQ_SAVE_PATH, exist_ok=True)\n",
        "create_sequences_and_save(df, user_ids, SEQ_LENGTH, save_path=SEQ_SAVE_PATH)"
      ],
      "metadata": {
        "id": "HlWu9jczYelr"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test, users_train, users_test = load_all_sequences_from_disk(SEQ_SAVE_PATH)"
      ],
      "metadata": {
        "id": "TX0yLsS_Yj0R"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "anchors, positives, negatives = create_triplets(X_train, users_train)\n",
        "triplet_encoder = contrastive_model(X_train.shape[1:], EMBEDDING_DIM)\n",
        "optimizer = tf.keras.optimizers.Adam(1e-3)"
      ],
      "metadata": {
        "id": "CaNI-rAaYkdp"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Исправленный блок обучения с EarlyStopping (в памяти)\n",
        "early_stopping = EarlyStopping(\n",
        "    monitor=\"loss\",\n",
        "    patience=2,\n",
        "    restore_best_weights=True,\n",
        "    mode=\"min\"\n",
        ")\n",
        "\n",
        "best_weights = None\n",
        "best_loss = float('inf')\n",
        "wait = 0\n",
        "\n",
        "for epoch in range(10):\n",
        "    with tf.GradientTape() as tape:\n",
        "        emb_a = triplet_encoder(anchors)\n",
        "        emb_p = triplet_encoder(positives)\n",
        "        emb_n = triplet_encoder(negatives)\n",
        "        loss = triplet_loss_fn(emb_a, emb_p, emb_n)\n",
        "    grads = tape.gradient(loss, triplet_encoder.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(grads, triplet_encoder.trainable_variables))\n",
        "\n",
        "    print(f\"contrastive epoch {epoch+1} // loss = {loss.numpy():.4f}\")\n",
        "\n",
        "    if loss < best_loss:\n",
        "        best_loss = loss\n",
        "        best_weights = triplet_encoder.get_weights()\n",
        "        wait = 0\n",
        "    else:\n",
        "        wait += 1\n",
        "        if wait >= early_stopping.patience:\n",
        "            print(f\"Ранняя остановка на эпохе {epoch+1}\")\n",
        "            triplet_encoder.set_weights(best_weights)\n",
        "            break\n",
        "\n",
        "if epoch + 1 < 10:\n",
        "    print(f\"Восстановлены лучшие веса с loss = {best_loss:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l6-oA6qFYnK9",
        "outputId": "d23325c0-4fbe-4af2-8dd4-58225e1d56f1"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "contrastive epoch 1 // loss = 0.0436\n",
            "contrastive epoch 2 // loss = 0.0398\n",
            "contrastive epoch 3 // loss = 0.0368\n",
            "contrastive epoch 4 // loss = 0.0344\n",
            "contrastive epoch 5 // loss = 0.0322\n",
            "contrastive epoch 6 // loss = 0.0300\n",
            "contrastive epoch 7 // loss = 0.0281\n",
            "contrastive epoch 8 // loss = 0.0263\n",
            "contrastive epoch 9 // loss = 0.0249\n",
            "contrastive epoch 10 // loss = 0.0239\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "user_embeddings_matrix = {}\n",
        "for uid in np.unique(users_train):\n",
        "    user_seqs = X_train[users_train == uid]\n",
        "    user_embs = triplet_encoder.predict(user_seqs, batch_size=1024)\n",
        "    user_embeddings_matrix[uid] = np.mean(user_embs, axis=0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YR-actdOYpuu",
        "outputId": "8664e325-5d72-4bc7-a2a8-947b6b943235"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step\n",
            "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step\n",
            "\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "user_embeddings_train = np.array([user_embeddings_matrix[uid] for uid in users_train])\n",
        "user_embeddings_test = np.array([user_embeddings_matrix[uid] for uid in users_test])"
      ],
      "metadata": {
        "id": "dgphkFZrYsCO"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_tf_dataset(generator):\n",
        "    output_signature = (\n",
        "        {\n",
        "            'macro_input': tf.TensorSpec(shape=(None, SEQ_LENGTH, len(features)), dtype=tf.float32),\n",
        "            'micro_input': tf.TensorSpec(shape=(None, SEQ_LENGTH, len(features)), dtype=tf.float32),\n",
        "            'user_input': tf.TensorSpec(shape=(None, EMBEDDING_DIM), dtype=tf.float32)\n",
        "        },\n",
        "        {\n",
        "            'macro_output': tf.TensorSpec(shape=(None,), dtype=tf.int32),\n",
        "            'micro_output': tf.TensorSpec(shape=(None, PRED_LENGTH, 2), dtype=tf.float32)\n",
        "        }\n",
        "    )\n",
        "    return tf.data.Dataset.from_generator(\n",
        "        lambda: generator,\n",
        "        output_signature=output_signature\n",
        "    )"
      ],
      "metadata": {
        "id": "qWcmQI9FJcSK"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = build_dual_scale_model(\n",
        "    macro_input_shape=(SEQ_LENGTH, len(features)),\n",
        "    micro_input_shape=(SEQ_LENGTH, len(features)),\n",
        "    num_macro_classes=len(le_500.classes_),\n",
        "    embedding_dim=EMBEDDING_DIM,\n",
        "    le_500=le_500\n",
        ")\n",
        "\n",
        "# Компиляция\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss={\n",
        "        'macro_output': 'sparse_categorical_crossentropy',\n",
        "        'micro_output': 'mse'\n",
        "    },\n",
        "    loss_weights={'macro_output': 0.3, 'micro_output': 0.7},\n",
        "    metrics={'micro_output': ['mae']}\n",
        ")"
      ],
      "metadata": {
        "id": "SNci6X1vYuFM"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(model.output_names)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fZX8Z1LSLZ_J",
        "outputId": "a839d80b-63eb-4c1d-9aef-939aba980c73"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ListWrapper(['functional_16', 'functional_17'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_gen = CombinedDataGenerator(X_train, X_train, user_embeddings_train, y_train, BATCH_SIZE)\n",
        "val_gen = CombinedDataGenerator(X_test, X_test, user_embeddings_test, y_test, BATCH_SIZE)"
      ],
      "metadata": {
        "id": "mLwXMnoxJfrn"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = create_tf_dataset(train_gen)\n",
        "val_dataset = create_tf_dataset(val_gen)"
      ],
      "metadata": {
        "id": "SCf7Cm7AJgum"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SAVE_PATH2 = \"/content/drive/My Drive/Colab Notebooks/my-model-v3/model.keras\"\n",
        "history = model.fit(\n",
        "    train_dataset,\n",
        "    validation_data=val_dataset,\n",
        "    epochs=5,\n",
        "    callbacks=[\n",
        "        EarlyStopping(patience=3, restore_best_weights=True),\n",
        "        ModelCheckpoint(SAVE_PATH2, save_best_only=True)\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 401
        },
        "id": "NggAoA9yJl1l",
        "outputId": "df57f8c1-fb07-4eed-e305-30e28988503d"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Expected keys ListWrapper(['functional_16', 'functional_17']) in loss dict, but found loss.keys()=['macro_output', 'micro_output']",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-89-dfc108a27d0f>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mSAVE_PATH2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/content/drive/My Drive/Colab Notebooks/my-model-v3/model.keras\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m history = model.fit(\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/trainers/compile_utils.py\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(self, y_true, y_pred)\u001b[0m\n\u001b[1;32m    583\u001b[0m                     ]\n\u001b[1;32m    584\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 585\u001b[0;31m                 raise ValueError(\n\u001b[0m\u001b[1;32m    586\u001b[0m                     \u001b[0;34mf\"Expected keys {self.output_names} in loss dict, but \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m                     \u001b[0;34mf\"found loss.keys()={list(self._user_loss.keys())}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Expected keys ListWrapper(['functional_16', 'functional_17']) in loss dict, but found loss.keys()=['macro_output', 'micro_output']"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, X_test, user_embeddings_test, y_test, le_500):\n",
        "    # Предсказание\n",
        "    macro_pred, micro_pred = model.predict([X_test, user_embeddings_test])\n",
        "\n",
        "    # Преобразование локальных координат в глобальные\n",
        "    cell_ids = np.argmax(macro_pred, axis=1)\n",
        "    cell_centers = np.array([h3.h3_to_geo(le_500.inverse_transform([cid])[0]) for cid in cell_ids])\n",
        "\n",
        "    # Расчет глобальных координат\n",
        "    global_pred = cell_centers[:, None, :] + micro_pred\n",
        "\n",
        "    # Расчет метрик\n",
        "    ade = np.mean(np.linalg.norm(global_pred - y_test_global, axis=2))\n",
        "    fde = np.mean(np.linalg.norm(global_pred[:, -1, :] - y_test_global[:, -1, :], axis=1))\n",
        "\n",
        "    print(f\"ADE: {ade:.2f} м | FDE: {fde:.2f} м\")\n",
        "    return ade, fde"
      ],
      "metadata": {
        "id": "Kzm_hEonv4qf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}